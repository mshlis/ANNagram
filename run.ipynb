{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, X1, Y, vocab_to_ind = pkl.load(open('../data/data.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_ind[' '] = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(vocab_to_ind, open('word_to_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0 = np.eye(X0.shape[1])\n",
    "Y0 = np.concatenate([Y0, np.zeros((2, X0.shape[1]))], axis=0)[np.newaxis,]\n",
    "Y0 = np.tile(Y0, (len(X0),1,1))\n",
    "Y0.shape\n",
    "\n",
    "l0, l1 = X0.shape[1], X1.shape[1]\n",
    "B = np.tile(np.arange(l0).reshape((1,-1)), (l1, 1)) \n",
    "B = -1e9 * (B < np.arange(l1)[:,np.newaxis]).astype(int)\n",
    "B = np.tile(B[np.newaxis,], (len(X0),1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/lib/python3/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1007 20:53:10.198968 140004382619456 deprecation_wrapper.py:119] From /home/mshlis/Projects/ANNagram/ANNagram/models.py:5: The name tf.keras.layers.CuDNNGRU is deprecated. Please use tf.compat.v1.keras.layers.CuDNNGRU instead.\n",
      "\n",
      "W1007 20:53:10.200144 140004382619456 deprecation_wrapper.py:119] From /home/mshlis/Projects/ANNagram/ANNagram/models.py:5: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n",
      "W1007 20:53:10.231902 140004382619456 deprecation.py:506] From /usr/lib/python3/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1007 20:53:10.242923 140004382619456 deprecation.py:506] From /usr/lib/python3/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "mask_meta = '<m>'\n",
    "model = models.pointer_transformer(alphabet_size=len(vocab_to_ind)+1,\n",
    "                                   blocks=2,\n",
    "                                   num_heads=4,\n",
    "                                   dim=64,\n",
    "                                   dropout=None,\n",
    "                                   mask_meta=vocab_to_ind[mask_meta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true[:, :, 2:3], 1.), tf.float32)\n",
    "    logits = tf.log(y_pred + 1e-9)\n",
    "    return - mask * y_true * logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.arange(len(Y0))\n",
    "np.random.shuffle(inds)\n",
    "\n",
    "Ntr = int(.7*len(Y0))\n",
    "train_inds = inds[:Ntr]\n",
    "test_inds = inds[Ntr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_h = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 230603 samples, validate on 98830 samples\n",
      "Epoch 1/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0681\n",
      "Epoch 00001: val_loss improved from inf to 0.06101, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 17s 76us/sample - loss: 0.0681 - val_loss: 0.0610\n",
      "Epoch 2/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0590\n",
      "Epoch 00002: val_loss improved from 0.06101 to 0.05808, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0590 - val_loss: 0.0581\n",
      "Epoch 3/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0572\n",
      "Epoch 00003: val_loss improved from 0.05808 to 0.05691, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0572 - val_loss: 0.0569\n",
      "Epoch 4/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0561\n",
      "Epoch 00004: val_loss improved from 0.05691 to 0.05607, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0561 - val_loss: 0.0561\n",
      "Epoch 5/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0553\n",
      "Epoch 00005: val_loss improved from 0.05607 to 0.05528, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0553 - val_loss: 0.0553\n",
      "Epoch 6/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0547\n",
      "Epoch 00006: val_loss improved from 0.05528 to 0.05485, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0547 - val_loss: 0.0549\n",
      "Epoch 7/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0542\n",
      "Epoch 00007: val_loss improved from 0.05485 to 0.05438, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0542 - val_loss: 0.0544\n",
      "Epoch 8/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0538\n",
      "Epoch 00008: val_loss improved from 0.05438 to 0.05402, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0538 - val_loss: 0.0540\n",
      "Epoch 9/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0534\n",
      "Epoch 00009: val_loss improved from 0.05402 to 0.05380, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0534 - val_loss: 0.0538\n",
      "Epoch 10/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0531\n",
      "Epoch 00010: val_loss did not improve from 0.05380\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0531 - val_loss: 0.0539\n",
      "Epoch 11/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0528\n",
      "Epoch 00011: val_loss improved from 0.05380 to 0.05337, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0528 - val_loss: 0.0534\n",
      "Epoch 12/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0526\n",
      "Epoch 00012: val_loss improved from 0.05337 to 0.05333, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0526 - val_loss: 0.0533\n",
      "Epoch 13/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0523\n",
      "Epoch 00013: val_loss improved from 0.05333 to 0.05285, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0523 - val_loss: 0.0529\n",
      "Epoch 14/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0521\n",
      "Epoch 00014: val_loss did not improve from 0.05285\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0521 - val_loss: 0.0529\n",
      "Epoch 15/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0519\n",
      "Epoch 00015: val_loss improved from 0.05285 to 0.05252, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0519 - val_loss: 0.0525\n",
      "Epoch 16/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0517\n",
      "Epoch 00016: val_loss improved from 0.05252 to 0.05241, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0517 - val_loss: 0.0524\n",
      "Epoch 17/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0515\n",
      "Epoch 00017: val_loss improved from 0.05241 to 0.05221, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0515 - val_loss: 0.0522\n",
      "Epoch 18/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0514\n",
      "Epoch 00018: val_loss improved from 0.05221 to 0.05216, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0514 - val_loss: 0.0522\n",
      "Epoch 19/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0512\n",
      "Epoch 00019: val_loss improved from 0.05216 to 0.05212, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0512 - val_loss: 0.0521\n",
      "Epoch 20/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0510\n",
      "Epoch 00020: val_loss improved from 0.05212 to 0.05199, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0511 - val_loss: 0.0520\n",
      "Epoch 21/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0509\n",
      "Epoch 00021: val_loss did not improve from 0.05199\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0509 - val_loss: 0.0522\n",
      "Epoch 22/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0508\n",
      "Epoch 00022: val_loss improved from 0.05199 to 0.05177, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0508 - val_loss: 0.0518\n",
      "Epoch 23/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0507\n",
      "Epoch 00023: val_loss did not improve from 0.05177\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0507 - val_loss: 0.0518\n",
      "Epoch 24/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0506\n",
      "Epoch 00024: val_loss improved from 0.05177 to 0.05175, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0506 - val_loss: 0.0517\n",
      "Epoch 25/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0504\n",
      "Epoch 00025: val_loss improved from 0.05175 to 0.05152, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0504 - val_loss: 0.0515\n",
      "Epoch 26/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0503\n",
      "Epoch 00026: val_loss did not improve from 0.05152\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0503 - val_loss: 0.0515\n",
      "Epoch 27/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0503\n",
      "Epoch 00027: val_loss did not improve from 0.05152\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0503 - val_loss: 0.0517\n",
      "Epoch 28/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0502\n",
      "Epoch 00028: val_loss improved from 0.05152 to 0.05151, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0502 - val_loss: 0.0515\n",
      "Epoch 29/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0501\n",
      "Epoch 00029: val_loss improved from 0.05151 to 0.05137, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0501 - val_loss: 0.0514\n",
      "Epoch 30/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0500\n",
      "Epoch 00030: val_loss improved from 0.05137 to 0.05131, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0500 - val_loss: 0.0513\n",
      "Epoch 31/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0499\n",
      "Epoch 00031: val_loss improved from 0.05131 to 0.05124, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0499 - val_loss: 0.0512\n",
      "Epoch 32/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0498\n",
      "Epoch 00032: val_loss did not improve from 0.05124\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0498 - val_loss: 0.0514\n",
      "Epoch 33/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0498\n",
      "Epoch 00033: val_loss did not improve from 0.05124\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0498 - val_loss: 0.0514\n",
      "Epoch 34/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0497\n",
      "Epoch 00034: val_loss did not improve from 0.05124\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0497 - val_loss: 0.0513\n",
      "Epoch 35/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0496\n",
      "Epoch 00035: val_loss did not improve from 0.05124\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0496 - val_loss: 0.0514\n",
      "Epoch 36/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0495\n",
      "Epoch 00036: val_loss improved from 0.05124 to 0.05114, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0495 - val_loss: 0.0511\n",
      "Epoch 37/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0495\n",
      "Epoch 00037: val_loss improved from 0.05114 to 0.05108, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0495 - val_loss: 0.0511\n",
      "Epoch 38/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0494\n",
      "Epoch 00038: val_loss improved from 0.05108 to 0.05107, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0494 - val_loss: 0.0511\n",
      "Epoch 39/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0493\n",
      "Epoch 00039: val_loss did not improve from 0.05107\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0493 - val_loss: 0.0511\n",
      "Epoch 40/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0493\n",
      "Epoch 00040: val_loss improved from 0.05107 to 0.05097, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0493 - val_loss: 0.0510\n",
      "Epoch 41/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0492\n",
      "Epoch 00041: val_loss improved from 0.05097 to 0.05093, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0492 - val_loss: 0.0509\n",
      "Epoch 42/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0492\n",
      "Epoch 00042: val_loss did not improve from 0.05093\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0492 - val_loss: 0.0510\n",
      "Epoch 43/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0491\n",
      "Epoch 00043: val_loss improved from 0.05093 to 0.05092, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0491 - val_loss: 0.0509\n",
      "Epoch 44/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0490\n",
      "Epoch 00044: val_loss did not improve from 0.05092\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0490 - val_loss: 0.0510\n",
      "Epoch 45/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0490\n",
      "Epoch 00045: val_loss did not improve from 0.05092\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0490 - val_loss: 0.0510\n",
      "Epoch 46/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0490\n",
      "Epoch 00046: val_loss improved from 0.05092 to 0.05087, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0490 - val_loss: 0.0509\n",
      "Epoch 47/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0489\n",
      "Epoch 00047: val_loss improved from 0.05087 to 0.05085, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0489 - val_loss: 0.0508\n",
      "Epoch 48/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0489\n",
      "Epoch 00048: val_loss did not improve from 0.05085\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0489 - val_loss: 0.0509\n",
      "Epoch 49/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0488\n",
      "Epoch 00049: val_loss did not improve from 0.05085\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0488 - val_loss: 0.0509\n",
      "Epoch 50/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0488\n",
      "Epoch 00050: val_loss improved from 0.05085 to 0.05083, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0488 - val_loss: 0.0508\n",
      "Epoch 51/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0488\n",
      "Epoch 00051: val_loss did not improve from 0.05083\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0488 - val_loss: 0.0508\n",
      "Epoch 52/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0487\n",
      "Epoch 00052: val_loss improved from 0.05083 to 0.05078, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0487 - val_loss: 0.0508\n",
      "Epoch 53/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0486\n",
      "Epoch 00053: val_loss improved from 0.05078 to 0.05076, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0486 - val_loss: 0.0508\n",
      "Epoch 54/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0486\n",
      "Epoch 00054: val_loss improved from 0.05076 to 0.05076, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0486 - val_loss: 0.0508\n",
      "Epoch 55/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0486\n",
      "Epoch 00055: val_loss did not improve from 0.05076\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0486 - val_loss: 0.0508\n",
      "Epoch 56/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0486\n",
      "Epoch 00056: val_loss did not improve from 0.05076\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0485 - val_loss: 0.0508\n",
      "Epoch 57/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0485\n",
      "Epoch 00057: val_loss did not improve from 0.05076\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0485 - val_loss: 0.0509\n",
      "Epoch 58/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0485\n",
      "Epoch 00058: val_loss did not improve from 0.05076\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0485 - val_loss: 0.0508\n",
      "Epoch 59/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0484\n",
      "Epoch 00059: val_loss did not improve from 0.05076\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0484 - val_loss: 0.0509\n",
      "Epoch 60/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0484\n",
      "Epoch 00060: val_loss improved from 0.05076 to 0.05073, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0484 - val_loss: 0.0507\n",
      "Epoch 61/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0484\n",
      "Epoch 00061: val_loss did not improve from 0.05073\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0484 - val_loss: 0.0507\n",
      "Epoch 62/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0483\n",
      "Epoch 00062: val_loss did not improve from 0.05073\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0483 - val_loss: 0.0508\n",
      "Epoch 63/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0483\n",
      "Epoch 00063: val_loss improved from 0.05073 to 0.05071, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0483 - val_loss: 0.0507\n",
      "Epoch 64/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0483\n",
      "Epoch 00064: val_loss improved from 0.05071 to 0.05068, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0483 - val_loss: 0.0507\n",
      "Epoch 65/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0483\n",
      "Epoch 00065: val_loss improved from 0.05068 to 0.05062, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0483 - val_loss: 0.0506\n",
      "Epoch 66/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0482\n",
      "Epoch 00066: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0482 - val_loss: 0.0508\n",
      "Epoch 67/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0482\n",
      "Epoch 00067: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0482 - val_loss: 0.0507\n",
      "Epoch 68/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0482\n",
      "Epoch 00068: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0482 - val_loss: 0.0507\n",
      "Epoch 69/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0481\n",
      "Epoch 00069: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0481 - val_loss: 0.0507\n",
      "Epoch 70/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0481\n",
      "Epoch 00070: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0481 - val_loss: 0.0507\n",
      "Epoch 71/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0481\n",
      "Epoch 00071: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0481 - val_loss: 0.0507\n",
      "Epoch 72/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0481\n",
      "Epoch 00072: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0481 - val_loss: 0.0507\n",
      "Epoch 73/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0481\n",
      "Epoch 00073: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0481 - val_loss: 0.0507\n",
      "Epoch 74/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0480\n",
      "Epoch 00074: val_loss did not improve from 0.05062\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0480 - val_loss: 0.0507\n",
      "Epoch 75/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0480\n",
      "Epoch 00075: val_loss improved from 0.05062 to 0.05059, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0480 - val_loss: 0.0506\n",
      "Epoch 76/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0480\n",
      "Epoch 00076: val_loss did not improve from 0.05059\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0480 - val_loss: 0.0507\n",
      "Epoch 77/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0480\n",
      "Epoch 00077: val_loss did not improve from 0.05059\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0480 - val_loss: 0.0506\n",
      "Epoch 78/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0479\n",
      "Epoch 00078: val_loss improved from 0.05059 to 0.05059, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0479 - val_loss: 0.0506\n",
      "Epoch 79/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0479\n",
      "Epoch 00079: val_loss did not improve from 0.05059\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0479 - val_loss: 0.0507\n",
      "Epoch 80/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0479\n",
      "Epoch 00080: val_loss did not improve from 0.05059\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0479 - val_loss: 0.0507\n",
      "Epoch 81/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0479\n",
      "Epoch 00081: val_loss improved from 0.05059 to 0.05058, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0479 - val_loss: 0.0506\n",
      "Epoch 82/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0479\n",
      "Epoch 00082: val_loss did not improve from 0.05058\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0479 - val_loss: 0.0506\n",
      "Epoch 83/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0478\n",
      "Epoch 00083: val_loss did not improve from 0.05058\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0478 - val_loss: 0.0507\n",
      "Epoch 84/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0478\n",
      "Epoch 00084: val_loss did not improve from 0.05058\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0478 - val_loss: 0.0507\n",
      "Epoch 85/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0478\n",
      "Epoch 00085: val_loss did not improve from 0.05058\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0478 - val_loss: 0.0508\n",
      "Epoch 86/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0478\n",
      "Epoch 00086: val_loss did not improve from 0.05058\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0478 - val_loss: 0.0506\n",
      "Epoch 87/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0478\n",
      "Epoch 00087: val_loss did not improve from 0.05058\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0478 - val_loss: 0.0508\n",
      "Epoch 88/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0478\n",
      "Epoch 00088: val_loss did not improve from 0.05058\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0478 - val_loss: 0.0506\n",
      "Epoch 89/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0477\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.05058 to 0.05057, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0477 - val_loss: 0.0506\n",
      "Epoch 90/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0470\n",
      "Epoch 00090: val_loss improved from 0.05057 to 0.05036, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 49us/sample - loss: 0.0470 - val_loss: 0.0504\n",
      "Epoch 91/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Epoch 00091: val_loss did not improve from 0.05036\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0469 - val_loss: 0.0504\n",
      "Epoch 92/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Epoch 00092: val_loss did not improve from 0.05036\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0469 - val_loss: 0.0504\n",
      "Epoch 93/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Epoch 00093: val_loss did not improve from 0.05036\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0469 - val_loss: 0.0504\n",
      "Epoch 94/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Epoch 00094: val_loss did not improve from 0.05036\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0469 - val_loss: 0.0504\n",
      "Epoch 95/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0469\n",
      "Epoch 00095: val_loss did not improve from 0.05036\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0469 - val_loss: 0.0505\n",
      "Epoch 96/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0468\n",
      "Epoch 00096: val_loss did not improve from 0.05036\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0468 - val_loss: 0.0504\n",
      "Epoch 97/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0468\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.05036\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0468 - val_loss: 0.0505\n",
      "Epoch 98/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0464\n",
      "Epoch 00098: val_loss improved from 0.05036 to 0.05034, saving model to ANN_weights_s3.h5\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0464 - val_loss: 0.0503\n",
      "Epoch 99/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0464\n",
      "Epoch 00099: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0464 - val_loss: 0.0504\n",
      "Epoch 100/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00100: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0463 - val_loss: 0.0504\n",
      "Epoch 101/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00101: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0463 - val_loss: 0.0504\n",
      "Epoch 102/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00102: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0463 - val_loss: 0.0505\n",
      "Epoch 103/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00103: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0463 - val_loss: 0.0505\n",
      "Epoch 104/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00104: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0463 - val_loss: 0.0505\n",
      "Epoch 105/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00105: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0463 - val_loss: 0.0506\n",
      "Epoch 106/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00106: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0463 - val_loss: 0.0505\n",
      "Epoch 107/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00107: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0463 - val_loss: 0.0505\n",
      "Epoch 108/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0463\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0463 - val_loss: 0.0505\n",
      "Epoch 109/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Epoch 00109: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0460 - val_loss: 0.0505\n",
      "Epoch 110/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Epoch 00110: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0460 - val_loss: 0.0505\n",
      "Epoch 111/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Epoch 00111: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0460 - val_loss: 0.0505\n",
      "Epoch 112/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Epoch 00112: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0460 - val_loss: 0.0506\n",
      "Epoch 113/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Epoch 00113: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0460 - val_loss: 0.0506\n",
      "Epoch 114/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0460\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0460 - val_loss: 0.0506\n",
      "Epoch 115/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0458\n",
      "Epoch 00115: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0458 - val_loss: 0.0506\n",
      "Epoch 116/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0458\n",
      "Epoch 00116: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0458 - val_loss: 0.0506\n",
      "Epoch 117/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0458\n",
      "Epoch 00117: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0458 - val_loss: 0.0506\n",
      "Epoch 118/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0458\n",
      "Epoch 00118: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0458 - val_loss: 0.0506\n",
      "Epoch 119/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0458\n",
      "Epoch 00119: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0458 - val_loss: 0.0507\n",
      "Epoch 120/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0458\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0458 - val_loss: 0.0506\n",
      "Epoch 121/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00121: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 122/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00122: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 123/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00123: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 124/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00124: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 125/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00125: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 126/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 127/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00127: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 128/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00128: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 129/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00129: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 130/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00130: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 131/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 132/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00132: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 133/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00133: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 134/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00134: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 135/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00135: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 136/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0457\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0457 - val_loss: 0.0507\n",
      "Epoch 137/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00137: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 138/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00138: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 139/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00139: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 140/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00140: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 141/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 142/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00142: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 143/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00143: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 47us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 144/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00144: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 145/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00145: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 146/150\n",
      "230400/230603 [============================>.] - ETA: 0s - loss: 0.0456\n",
      "Epoch 00146: val_loss did not improve from 0.05034\n",
      "230603/230603 [==============================] - 11s 48us/sample - loss: 0.0456 - val_loss: 0.0507\n",
      "Epoch 147/150\n",
      " 89088/230603 [==========>...................] - ETA: 5s - loss: 0.0456"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f44aa1b9969a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     callbacks=[rlop, es, mc])\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "rlop = ReduceLROnPlateau(monitor='loss', verbose=1, factor=.5, patience=5)\n",
    "es = EarlyStopping(monitor='loss', verbose=1, patience=12)\n",
    "mc = ModelCheckpoint('ANN_weights_s3.h5', verbose=1, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model.compile(keras.optimizers.Adam(1e-3), loss)\n",
    "history = model.fit([X1[train_inds], X0[train_inds], B[train_inds]], Y0[train_inds],\n",
    "                    batch_size=1024, epochs=150,\n",
    "                    validation_data=([X1[test_inds], X0[test_inds], B[test_inds]], Y0[test_inds]),\n",
    "                    callbacks=[rlop, es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('ANN_weights_s3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_meta = '<s>'\n",
    "end_meta = '<e>'\n",
    "\n",
    "start = [start_meta]\n",
    "s0 = np.array([vocab_to_ind[c] for c in start]).reshape((1,-1))\n",
    "\n",
    "letters = list('ghost')\n",
    "letter_inds = np.array([vocab_to_ind[c] for c in letters]).reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>g\n",
      "<s>go\n",
      "<s>got\n",
      "<s>goth\n",
      "<s>goths\n"
     ]
    }
   ],
   "source": [
    "from utils import \n",
    "\n",
    "ind_to_vocab = {v:k for k,v in  vocab_to_ind.items()}\n",
    "def write_out(s, ind_to_vocab):\n",
    "    string = ''\n",
    "    for n in s[0]:\n",
    "        string += ind_to_vocab[n]\n",
    "    return string\n",
    "\n",
    "for _ in range(len(letters)):\n",
    "    nxt_p = model.predict([s0, letter_inds])[0,-1]\n",
    "    nxt_n = np.random.choice(len(nxt_p), p=nxt_p) # nxt_p.argmax()\n",
    "    nxt_c = letters[nxt_n]\n",
    "    s0 = np.concatenate([s0, vocab_to_ind[nxt_c] * np.ones((1,1))], -1)\n",
    "    print(write_out(s0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 6 6 6]\n",
      " [5 5 5 5]\n",
      " [4 4 4 4]\n",
      " [3 3 3 3]\n",
      " [2 2 2 2]\n",
      " [1 1 1 1]]\n",
      "[[0 0 0 0]\n",
      " [1 0 1 0]\n",
      " [1 0 1 0]\n",
      " [1 0 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]]\n",
      "[[0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "z = tf.expand_dims(tf.range(6)[::-1], -1)\n",
    "z = tf.tile(z, (1,4))\n",
    "\n",
    "x0 = tf.constant([[2,1,2,3]])\n",
    "x1 = tf.constant([[0,2,2,3,1,4]])\n",
    "\n",
    "chk_bin = tf.equal(tf.transpose(x1), x0)\n",
    "chk_bin = tf.cast(chk_bin, tf.int32)\n",
    "chk = (z+1)*chk_bin\n",
    "\n",
    "bal = tf.expand_dims(1+tf.range(4), 0)\n",
    "chk_bal = bal*chk_bin\n",
    "#chk_bal = tf.reduce_max(chk_bal, -1)\n",
    "chk_bal = tf.greater_equal(chk_bal, tf.reduce_max(chk_bal, -1, keepdims=True))\n",
    "chk_bal = chk_bin*tf.cast(chk_bal, tf.int32)\n",
    "\n",
    "chk = tf.less_equal(z+1, tf.reduce_max(chk, 0))\n",
    "chk = tf.cast(chk, tf.int32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z+1))\n",
    "    print(sess.run(chk))\n",
    "    print(sess.run(chk_bal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f : p=[0.32234677]\n",
      "u : p=[0.27813451]\n",
      "c : p=[0.25998696]\n",
      "i : p=[0.07820435]\n",
      "n : p=[0.06132741]\n",
      "\n",
      "fu : p=[0.34347665]\n",
      "co : p=[0.27862752]\n",
      "un : p=[0.15545223]\n",
      "un : p=[0.15545223]\n",
      "no : p=[0.06699137]\n",
      "\n",
      "cou : p=[0.30867757]\n",
      "fun : p=[0.2180532]\n",
      "fun : p=[0.2180532]\n",
      "unf : p=[0.12760802]\n",
      "unf : p=[0.12760801]\n",
      "\n",
      "func : p=[0.24791789]\n",
      "func : p=[0.24791789]\n",
      "coun : p=[0.17981615]\n",
      "coun : p=[0.17981615]\n",
      "unfo : p=[0.14453193]\n",
      "\n",
      "funct : p=[0.2651667]\n",
      "funct : p=[0.26516667]\n",
      "counf : p=[0.1679193]\n",
      "counf : p=[0.1679193]\n",
      "unfoc : p=[0.13382802]\n",
      "\n",
      "functi : p=[0.28327749]\n",
      "functi : p=[0.28327742]\n",
      "counfi : p=[0.17864285]\n",
      "counfi : p=[0.17864285]\n",
      "unfoci : p=[0.07615938]\n",
      "\n",
      "functio : p=[0.28309446]\n",
      "functio : p=[0.28309439]\n",
      "counfin : p=[0.17879373]\n",
      "counfin : p=[0.17879373]\n",
      "unfocin : p=[0.07622369]\n",
      "\n",
      "function : p=[0.28309446]\n",
      "function : p=[0.28309439]\n",
      "counfint : p=[0.17879373]\n",
      "counfint : p=[0.17879373]\n",
      "unfocint : p=[0.07622369]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "ind_to_vocab = {v:k for k,v in vocab_to_ind.items()}\n",
    "letters = list('function')\n",
    "letter_inds = np.array([vocab_to_ind[c] for c in letters]).reshape((1,-1))\n",
    "utils.beam_search(letter_inds, model.predict, vocab_to_ind, beam_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 6)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22599305, 0.21628883, 0.03875758, 0.04105627, 0.42614967,\n",
       "       0.0517546 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chk[0,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
